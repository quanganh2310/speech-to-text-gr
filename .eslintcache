[{"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\webaudio-l16-stream.js":"1","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\speaker-stream.js":"2","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\recognize-microphone.js":"3","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\App.js":"4","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\recognize-stream.js":"5","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\result-stream.js":"6","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\format-stream.js":"7","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\index.js":"8","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\components\\ModelDropdown.js":"9","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\renderer.js":"10","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\reportWebVitals.js":"11","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\util\\querystring.js":"12","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\content-type.js":"13","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\no-timestamps.js":"14","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\writable-element-stream.js":"15","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\util\\process-user-parameters.js":"16","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\to-promise.js":"17"},{"size":7174,"mtime":1614085623679,"results":"18","hashOfConfig":"19"},{"size":12132,"mtime":1614085623610,"results":"20","hashOfConfig":"19"},{"size":7642,"mtime":1616949258996,"results":"21","hashOfConfig":"19"},{"size":9527,"mtime":1616990438192,"results":"22","hashOfConfig":"19"},{"size":17591,"mtime":1614092055945,"results":"23","hashOfConfig":"19"},{"size":1717,"mtime":1614085623606,"results":"24","hashOfConfig":"19"},{"size":5224,"mtime":1614085623386,"results":"25","hashOfConfig":"19"},{"size":500,"mtime":1614397174744,"results":"26","hashOfConfig":"19"},{"size":1687,"mtime":1614526620997,"results":"27","hashOfConfig":"19"},{"size":730,"mtime":1616949585428,"results":"28","hashOfConfig":"19"},{"size":362,"mtime":1609599939708,"results":"29","hashOfConfig":"19"},{"size":749,"mtime":1614088623645,"results":"30","hashOfConfig":"19"},{"size":2105,"mtime":1614085623304,"results":"31","hashOfConfig":"19"},{"size":437,"mtime":1614085623492,"results":"32","hashOfConfig":"19"},{"size":2343,"mtime":1614085623694,"results":"33","hashOfConfig":"19"},{"size":1641,"mtime":1614088623608,"results":"34","hashOfConfig":"19"},{"size":778,"mtime":1614085623649,"results":"35","hashOfConfig":"19"},{"filePath":"36","messages":"37","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"38"},"fq9jch",{"filePath":"39","messages":"40","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"41","usedDeprecatedRules":"42"},{"filePath":"43","messages":"44","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":null},{"filePath":"45","messages":"46","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"47","messages":"48","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":1,"source":"49"},{"filePath":"50","messages":"51","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"52","usedDeprecatedRules":"53"},{"filePath":"54","messages":"55","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"56","usedDeprecatedRules":"57"},{"filePath":"58","messages":"59","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"61","messages":"62","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"63","messages":"64","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"65","messages":"66","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"67"},{"filePath":"68","messages":"69","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"70","usedDeprecatedRules":"71"},{"filePath":"72","messages":"73","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"74","usedDeprecatedRules":"75"},{"filePath":"76","messages":"77","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"78"},{"filePath":"79","messages":"80","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":"81","usedDeprecatedRules":"67"},{"filePath":"82","messages":"83","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"42"},{"filePath":"84","messages":"85","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":1,"source":null},"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\webaudio-l16-stream.js",["86"],"'use strict';\n\nvar { Transform } = require('readable-stream');\nvar util = require('util');\nvar defaults = require('defaults');\n// some versions of the buffer browser lib don't support Buffer.from (such as the one included by the current version of express-browserify)\nvar bufferFrom = require('buffer-from');\n\nvar TARGET_SAMPLE_RATE = 16000;\n/**\n * Transforms Buffers or AudioBuffers into a binary stream of l16 (raw wav) audio, downsampling in the process.\n *\n * The watson speech-to-text service works on 16kHz and internally downsamples audio received at higher samplerates.\n * WebAudio is usually 44.1kHz or 48kHz, so downsampling here reduces bandwidth usage by ~2/3.\n *\n * Format event + stream can be combined with https://www.npmjs.com/package/wav to generate a wav file with a proper header\n *\n * Todo: support multi-channel audio (for use with <audio>/<video> elements) - will require interleaving audio channels\n *\n * @param {Object} options\n * @constructor\n */\nfunction WebAudioL16Stream(options) {\n  options = this.options = defaults(options, {\n    sourceSampleRate: 48000,\n    downsample: true\n  });\n\n  Transform.call(this, options);\n\n  this.bufferUnusedSamples = [];\n\n  if (options.objectMode || options.writableObjectMode) {\n    this._transform = this.handleFirstAudioBuffer;\n  } else {\n    this._transform = this.transformBuffer;\n    process.nextTick(this.emitFormat.bind(this));\n  }\n}\nutil.inherits(WebAudioL16Stream, Transform);\n\nWebAudioL16Stream.prototype.emitFormat = function emitFormat() {\n  this.emit('format', {\n    channels: 1,\n    bitDepth: 16,\n    sampleRate: this.options.downsample ? TARGET_SAMPLE_RATE : this.options.sourceSampleRate,\n    signed: true,\n    float: false\n  });\n};\n\n/**\n * Downsamples WebAudio to 16 kHz.\n *\n * Browsers can downsample WebAudio natively with OfflineAudioContext's but it was designed for non-streaming use and\n * requires a new context for each AudioBuffer. Firefox can handle this, but chrome (v47) crashes after a few minutes.\n * So, we'll do it in JS for now.\n *\n * This really belongs in it's own stream, but there's no way to create new AudioBuffer instances from JS, so its\n * fairly coupled to the wav conversion code.\n *\n * @param  {AudioBuffer} bufferNewSamples Microphone/MediaElement audio chunk\n * @return {Float32Array} 'audio/l16' chunk\n */\nWebAudioL16Stream.prototype.downsample = function downsample(bufferNewSamples) {\n  var buffer = null;\n  var newSamples = bufferNewSamples.length;\n  var unusedSamples = this.bufferUnusedSamples.length;\n  var i;\n  var offset;\n\n  if (unusedSamples > 0) {\n    buffer = new Float32Array(unusedSamples + newSamples);\n    for (i = 0; i < unusedSamples; ++i) {\n      buffer[i] = this.bufferUnusedSamples[i];\n    }\n    for (i = 0; i < newSamples; ++i) {\n      buffer[unusedSamples + i] = bufferNewSamples[i];\n    }\n  } else {\n    buffer = bufferNewSamples;\n  }\n\n  // Downsampling and low-pass filter:\n  // Input audio is typically 44.1kHz or 48kHz, this downsamples it to 16kHz.\n  // It uses a FIR (finite impulse response) Filter to remove (or, at least attinuate)\n  // audio frequencies > ~8kHz because sampled audio cannot accurately represent\n  // frequiencies greater than half of the sample rate.\n  // (Human voice tops out at < 4kHz, so nothing important is lost for transcription.)\n  // See http://dsp.stackexchange.com/a/37475/26392 for a good explination of this code.\n  var filter = [\n    -0.037935,\n    -0.00089024,\n    0.040173,\n    0.019989,\n    0.0047792,\n    -0.058675,\n    -0.056487,\n    -0.0040653,\n    0.14527,\n    0.26927,\n    0.33913,\n    0.26927,\n    0.14527,\n    -0.0040653,\n    -0.056487,\n    -0.058675,\n    0.0047792,\n    0.019989,\n    0.040173,\n    -0.00089024,\n    -0.037935\n  ];\n  var samplingRateRatio = this.options.sourceSampleRate / TARGET_SAMPLE_RATE;\n  var nOutputSamples = Math.floor((buffer.length - filter.length) / samplingRateRatio) + 1;\n  var outputBuffer = new Float32Array(nOutputSamples);\n\n  for (i = 0; i < outputBuffer.length; i++) {\n    offset = Math.round(samplingRateRatio * i);\n    var sample = 0;\n    for (var j = 0; j < filter.length; ++j) {\n      sample += buffer[offset + j] * filter[j];\n    }\n    outputBuffer[i] = sample;\n  }\n\n  var indexSampleAfterLastUsed = Math.round(samplingRateRatio * i);\n  var remaining = buffer.length - indexSampleAfterLastUsed;\n  if (remaining > 0) {\n    this.bufferUnusedSamples = new Float32Array(remaining);\n    for (i = 0; i < remaining; ++i) {\n      this.bufferUnusedSamples[i] = buffer[indexSampleAfterLastUsed + i];\n    }\n  } else {\n    this.bufferUnusedSamples = new Float32Array(0);\n  }\n\n  return outputBuffer;\n};\n\n/**\n * Accepts a Float32Array of audio data and converts it to a Buffer of l16 audio data (raw wav)\n *\n * Explanation for the math: The raw values captured from the Web Audio API are\n * in 32-bit Floating Point, between -1 and 1 (per the specification).\n * The values for 16-bit PCM range between -32768 and +32767 (16-bit signed integer).\n * Filter & combine samples to reduce frequency, then multiply to by 0x7FFF (32767) to convert.\n * Store in little endian.\n *\n * @param {Float32Array} input\n * @return {Buffer}\n */\nWebAudioL16Stream.prototype.floatTo16BitPCM = function(input) {\n  var output = new DataView(new ArrayBuffer(input.length * 2)); // length is in bytes (8-bit), so *2 to get 16-bit length\n  for (var i = 0; i < input.length; i++) {\n    var multiplier = input[i] < 0 ? 0x8000 : 0x7fff; // 16-bit signed range is -32768 to 32767\n    output.setInt16(i * 2, (input[i] * multiplier) | 0, true); // index, value (\"| 0\" = convert to 32-bit int, round towards 0), littleEndian.\n  }\n  return bufferFrom(output.buffer);\n};\n\n/**\n * Does some one-time setup to grab sampleRate and emit format, then sets _transform to the actual audio buffer handler and calls it.\n * @param {AudioBuffer} audioBuffer\n * @param {String} encoding\n * @param {Function} next\n */\nWebAudioL16Stream.prototype.handleFirstAudioBuffer = function handleFirstAudioBuffer(audioBuffer, encoding, next) {\n  this.options.sourceSampleRate = audioBuffer.sampleRate;\n  this.emitFormat();\n  this._transform = this.transformAudioBuffer;\n  this._transform(audioBuffer, encoding, next);\n};\n\n/**\n * Accepts an AudioBuffer (for objectMode), then downsamples to 16000 and converts to a 16-bit pcm\n *\n * @param {AudioBuffer} audioBuffer\n * @param {String} encoding\n * @param {Function} next\n */\nWebAudioL16Stream.prototype.transformAudioBuffer = function(audioBuffer, encoding, next) {\n  var source = audioBuffer.getChannelData(0);\n  if (this.options.downsample) {\n    source = this.downsample(source);\n  }\n  this.push(this.floatTo16BitPCM(source));\n  next();\n};\n\n/**\n * Accepts a Buffer (for binary mode), then downsamples to 16000 and converts to a 16-bit pcm\n *\n * @param {Buffer} nodebuffer\n * @param {String} encoding\n * @param {Function} next\n */\nWebAudioL16Stream.prototype.transformBuffer = function(nodebuffer, encoding, next) {\n  var source = new Float32Array(nodebuffer.buffer);\n  if (this.options.downsample) {\n    source = this.downsample(source);\n  }\n  this.push(this.floatTo16BitPCM(source));\n  next();\n};\n// new Float32Array(nodebuffer.buffer)\n\nmodule.exports = WebAudioL16Stream;\n","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\speaker-stream.js",["87"],"/**\n * Copyright 2014 IBM Corp. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n'use strict';\n\nvar { Transform } = require('readable-stream');\nvar util = require('util');\nvar pullAllWith = require('lodash.pullallwith');\nvar noTimestamps = require('./no-timestamps');\nvar clone = require('clone');\n\n/**\n * Object-Mode stream that splits up results by speaker.\n *\n * Output format is similar to existing results formats, but with an extra speaker field,\n *\n * Output results array will usually contain multiple results.\n * All results are interim until the final batch; the text may change (if options.speakerlessInterim is enabled) or move from one interim result to another.\n *\n * Keywords, words_alternatives, and other features may appear on results that come slightly earlier than the timestamp due to the way things are split up.\n *\n * Ignores interim results from the service unless options.speakerlessInterim is enabled.\n *\n * @constructor\n * @param {Object} options\n * @param {boolean} [options.speakerlessInterim=false] - emit interim results before initial speaker has been identified (allows UI to update more quickly)\n */\nfunction SpeakerStream(options) {\n  options = options || {};\n  options.objectMode = true;\n  this.options = options;\n  Transform.call(this, options);\n  /**\n   * timestamps is a 2-d array.\n   * The sub-array is [word, from time, to time]\n   * Example:\n   * [\n       [\"Yes\", 28.92, 29.17],\n       [\"that's\", 29.17, 29.37],\n       [\"right\", 29.37, 29.64]\n    ]\n   * @type {Array<Array>}\n   * @private\n   */\n  this.results = [];\n  /**\n   * speaker_labels is an array of objects.\n   * Example:\n   * [{\n      \"from\": 28.92,\n      \"to\": 29.17,\n      \"speaker\": 1,\n      \"confidence\": 0.641,\n      \"final\": false\n    }, {\n      \"from\": 29.17,\n      \"to\": 29.37,\n      \"speaker\": 1,\n      \"confidence\": 0.641,\n      \"final\": false\n    }, {\n      \"from\": 29.37,\n      \"to\": 29.64,\n      \"speaker\": 1,\n      \"confidence\": 0.641,\n      \"final\": false\n    }]\n   * @type {Array<Object>}\n   * @private\n   */\n  this.speaker_labels = [];\n\n  this.mismatchErrorEmitted = false;\n\n  // flag to signal that labels were recieved before results, and therefore\n  // the stream needs to emit on the next batch of final results\n  this.extraLabels = false;\n}\nutil.inherits(SpeakerStream, Transform);\n\nSpeakerStream.prototype.isFinal = function() {\n  return this.speaker_labels.length && this.speaker_labels[this.speaker_labels.length - 1].final;\n};\n\n// positions in the timestamps 2d array\nvar WORD = 0;\nvar FROM = 1;\nvar TO = 2;\n\nSpeakerStream.ERROR_MISMATCH = 'MISMATCH';\n\n/**\n * Builds a results object with everything we've got so far\n * @return {*}\n */\nSpeakerStream.prototype.buildMessage = function() {\n  var final = this.isFinal();\n  this.extraLabels = false;\n\n  // first match all speaker_labeles to the appropriate word and result\n  // assumes that each speaker_label will have a matching word timestamp at the same index\n  // stops processing and emits an error if this assumption is violated\n  var resultIndex = 0;\n  var timestampIndex = -1;\n  var words = this.speaker_labels.map(\n    // eslint-disable-next-line camelcase\n    function(speaker_label) {\n      var result = this.results[resultIndex];\n      timestampIndex++;\n      var timestamp = result.alternatives[0].timestamps[timestampIndex];\n      if (!timestamp) {\n        timestampIndex = 0;\n        resultIndex++;\n        result = this.results[resultIndex];\n        timestamp = result && result.alternatives[0].timestamps[timestampIndex];\n      }\n      if (!timestamp) {\n        // this shouldn't happen normally, but the TimingStream could inadvertently cause a\n        // speaker_labels to be emitted before a result\n        this.extraLabels = true;\n        return null;\n      }\n      if (timestamp[FROM] !== speaker_label.from || timestamp[TO] !== speaker_label.to) {\n        if (!this.mismatchErrorEmitted) {\n          var err = new Error('Mismatch between speaker_label and word timestamp');\n          err.name = SpeakerStream.ERROR_MISMATCH;\n          // eslint-disable-next-line camelcase\n          err.speaker_label = speaker_label;\n          err.timestamp = timestamp;\n          // eslint-disable-next-line camelcase\n          err.speaker_labels = this.speaker_labels;\n          err.results = this.results;\n          this.emit('error', err);\n          this.mismatchErrorEmitted = true; // If one is off, then a bunch probably are. Just emit one error.\n        }\n        return null;\n      }\n      return {\n        timestamp: timestamp,\n        speaker: speaker_label.speaker,\n        result: result\n      };\n    },\n    this\n  );\n\n  // assume that there's nothing new to emit right now,\n  // wait for new results to match our new labels\n  if (this.extraLabels) {\n    return;\n  }\n\n  // filter out any nulls\n  words = words.filter(function(w) {\n    return w;\n  });\n\n  // group the words together into utterances by speaker\n  var utterances = words.reduce(function(arr, word) {\n    var utterance = arr[arr.length - 1];\n    // any time the speaker changes or the (original) result changes, create a new utterance\n    if (!utterance || utterance.speaker !== word.speaker || utterance.result !== word.result) {\n      utterance = {\n        speaker: word.speaker,\n        timestamps: [word.timestamp],\n        result: word.result\n      };\n      // and add it to the list\n      arr.push(utterance);\n    } else {\n      // otherwise just append the current word to the current result\n      utterance.timestamps.push(word.timestamp);\n    }\n    return arr;\n  }, []);\n\n  // create new results\n  var results = utterances.map(function(utterance, i) {\n    // if this is the first usage of this result, clone the original (to keep keywords and such)\n    // otherwise create a new one\n    var result;\n    var lastUtterance = utterances[i - 1] || {};\n    if (utterance.result === lastUtterance.result) {\n      result = { alternatives: [{}] };\n    } else {\n      result = clone(utterance.result);\n    }\n\n    // update the result object\n    // set the speaker\n    result.speaker = utterance.speaker;\n    // overwrite the transcript and timestamps on the first alternative\n    var alt = result.alternatives[0];\n    alt.transcript =\n      utterance.timestamps\n        .map(function(ts) {\n          return ts[WORD];\n        })\n        .join(' ') + ' ';\n    alt.timestamps = utterance.timestamps;\n    // overwrite the final value\n    result.final = final;\n\n    var start = utterance.timestamps[0][1];\n    var end = utterance.timestamps[utterance.timestamps.length - 1][2];\n\n    // overwrite the word_alternatives\n    if (utterance.result.word_alternatives) {\n      var alts = utterance.result.word_alternatives.filter(function(walt) {\n        return walt.start_time >= start && walt.end_time <= end;\n      });\n      result.word_alternatives = alts;\n    }\n\n    // overwrite the keywords spotted\n    /* eslint-disable camelcase */\n    var original_keywords_result = utterance.result.keywords_result;\n    if (original_keywords_result) {\n      var keywords_result = {};\n      Object.keys(original_keywords_result).forEach(function(keyword) {\n        var spottings = original_keywords_result[keyword].filter(function(spotting) {\n          return spotting.start_time >= start && spotting.end_time <= end;\n        });\n        if (spottings.length) {\n          keywords_result[keyword] = spottings;\n        }\n      });\n      result.keywords_result = keywords_result;\n    }\n    /* eslint-enable camelcase */\n\n    return result;\n  });\n\n  // result_index is always 0 because the results always includes the entire conversation so far.\n  return { results: results, result_index: 0 };\n};\n\n/**\n * Captures the timestamps out of results or errors if timestamps are missing\n * @param {Object} data\n */\nSpeakerStream.prototype.handleResults = function(data) {\n  if (noTimestamps(data)) {\n    var err = new Error('SpeakerStream requires that timestamps and speaker_labels be enabled');\n    err.name = noTimestamps.ERROR_NO_TIMESTAMPS;\n    this.emit('error', err);\n    return;\n  }\n  data.results\n    .filter(function(result) {\n      return result.final;\n    })\n    .forEach(function(result) {\n      this.results.push(result);\n    }, this);\n};\n\n// sorts by start time and then end time\nSpeakerStream.speakerLabelsSorter = function(a, b) {\n  if (a.from === b.from) {\n    if (a.to === b.to) {\n      return 0;\n    }\n    return a.to < b.to ? -1 : 1;\n  }\n  return a.from < b.from ? -1 : 1;\n};\n\n/**\n * Only the very last labeled word gets final: true. Up until that point, all speaker_labels are considered interim and\n * may be repeated with a new speaker selected in a later set of speaker_labels.\n *\n * @private\n * @param {Object} data\n */\nSpeakerStream.prototype.handleSpeakerLabels = function(data) {\n  var speaker_labels = data.speaker_labels; // eslint-disable-line camelcase\n\n  // remove any values from the old speaker_labels that are duplicated in the new set\n  pullAllWith(this.speaker_labels, speaker_labels, function(old, nw) {\n    return old.from === nw.from && old.to === nw.to;\n  });\n\n  // next append the new labels to the remaining old ones\n  this.speaker_labels.push.apply(this.speaker_labels, data.speaker_labels);\n\n  // finally, ensure the list is still sorted chronologically\n  this.speaker_labels.sort(SpeakerStream.speakerLabelsSorter);\n};\n\nSpeakerStream.prototype._transform = function(data, encoding, next) {\n  var message;\n  if (Array.isArray(data.results)) {\n    this.handleResults(data);\n    if (this.options.speakerlessInterim && data.results.length && data.results[0].final === false) {\n      message = this.buildMessage();\n      message.results = message.results.concat(data.results);\n    }\n    // clean up if things got out of order\n    if (this.extraLabels && data.results.length && data.results[0].final === true) {\n      message = this.buildMessage();\n    }\n  }\n  if (Array.isArray(data.speaker_labels)) {\n    this.handleSpeakerLabels(data);\n    message = this.buildMessage();\n  }\n  if (message) {\n    /**\n     * Emit an object similar to the normal results object, only with multiple entries in the results Array (a new one\n     * each time the speaker changes), and with a speaker field on the results.\n     *\n     * result_index is always 0 because the results always includes the entire conversation so far.\n     *\n     * @event SpeakerStream#data\n     * @param {Object} results-format message with multiple results and an extra speaker field on each result\n     */\n    this.push(message);\n  }\n  next();\n};\n\n/**\n * catches cases where speaker_labels was not enabled and internal errors that cause data loss\n *\n * @param {Function} done\n * @private\n */\nSpeakerStream.prototype._flush = function(done) {\n  var timestamps = this.results\n    .map(function(r) {\n      return r.alternatives[0].timestamps;\n    })\n    .reduce(function(a, b) {\n      return a.concat(b);\n    }, []);\n  if (timestamps.length !== this.speaker_labels.length) {\n    var msg;\n    if (timestamps.length && !this.speaker_labels.length) {\n      msg = 'No speaker_labels found. SpeakerStream requires speaker_labels to be enabled.';\n    } else {\n      msg =\n        'Mismatch between number of word timestamps (' +\n        timestamps.length +\n        ') and number of speaker_labels (' +\n        this.speaker_labels.length +\n        ') - some data may be lost.';\n    }\n    var err = new Error(msg);\n    err.name = SpeakerStream.ERROR_MISMATCH;\n    err.speaker_labels = this.speaker_labels;\n    err.results = this.results;\n    this.emit('error', err);\n  }\n  done();\n};\n\nSpeakerStream.prototype.promise = require('./to-promise');\n\nmodule.exports = SpeakerStream;\n",["88","89"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\recognize-microphone.js",["90"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\App.js",["91","92"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\recognize-stream.js",["93","94","95"],"/**\n * Copyright 2014 IBM Corp. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n'use strict';\n\nvar { Duplex } = require('readable-stream');\nvar util = require('util');\nvar W3CWebSocket = require('websocket').w3cwebsocket;\nvar contentType = require('./content-type');\nvar processUserParameters = require('../util/process-user-parameters.js');\nvar qs = require('../util/querystring.js');\n\n/**\n * pipe()-able Node.js Duplex stream - accepts binary audio and emits text/objects in it's `data` events.\n *\n * Uses WebSockets under the hood. For audio with no recognizable speech, no `data` events are emitted.\n *\n * By default, only finalized text is emitted in the data events, however when `objectMode`/`readableObjectMode` and `interim_results` are enabled, both interim and final results objects are emitted.\n * WriteableElementStream uses this, for example, to live-update the DOM with word-by-word transcriptions.\n *\n * Note that the WebSocket connection is not established until the first chunk of data is recieved. This allows for auto-detection of content type (for wav/flac/opus audio).\n *\n * @param {Options} options\n * @param {string} [options.url] - Base url for service (default='wss://stream.watsonplatform.net/speech-to-text/api')\n * @param {OutgoingHttpHeaders} [options.headers] - Only works in Node.js, not in browsers. Allows for custom headers to be set, including an Authorization header (preventing the need for auth tokens)\n * @param {boolean} [options.readableObjectMode] - Emit `result` objects instead of string Buffers for the `data` events. Does not affect input (which must be binary)\n * @param {boolean} [options.objectMode] - Alias for readableObjectMode\n * @param {string} [options.accessToken] - Bearer token to put in query string\n * @param {string} [options.model] - The identifier of the model that is to be used for all recognition requests sent over the connection\n * @param {string} [options.languageCustomizationId] - The customization ID (GUID) of a custom language model that is to be used for all requests sent over the connection\n * @param {string} [options.acousticCustomizationId] - The customization ID (GUID) of a custom acoustic model that is to be used for the request\n * @param {string} [options.baseModelVersion] - The version of the specified base model that is to be used for all requests sent over the connection\n * @param {boolean} [options.xWatsonLearningOptOut] - Indicates whether IBM can use data that is sent over the connection to improve the service for future users (default=false)\n * @param {string} [options.xWatsonMetadata] - Associates a customer ID with all data that is passed over the connection. The parameter accepts the argument customer_id={id}, where {id} is a random or generic string that is to be associated with the data\n * @param {string} [options.contentType] - The format (MIME type) of the audio\n * @param {number} [options.customizationWeight] - Tell the service how much weight to give to words from the custom language model compared to those from the base model for the current request\n * @param {number} [options.inactivityTimeout] - The time in seconds after which, if only silence (no speech) is detected in the audio, the connection is closed (default=30)\n * @param {boolean} [options.interimResults] - If true, the service returns interim results as a stream of JSON SpeechRecognitionResults objects (default=false)\n * @param {string[]} [options.keywords] - An array of keyword strings to spot in the audio\n * @param {number} [options.keywordsThreshold] - A confidence value that is the lower bound for spotting a keyword\n * @param {number} [options.maxAlternatives] - The maximum number of alternative transcripts that the service is to return (default=1)\n * @param {number} [options.wordAlternativesThreshold] - A confidence value that is the lower bound for identifying a hypothesis as a possible word alternative\n * @param {boolean} [options.wordConfidence] - If true, the service returns a confidence measure in the range of 0.0 to 1.0 for each word (default=false)\n * @param {boolean} [options.timestamps] - If true, the service returns time alignment for each word (default=false)\n * @param {boolean} [options.profanityFilter] - If true, the service filters profanity from all output except for keyword results by replacing inappropriate words with a series of asterisks (default=true)\n * @param {boolean} [options.smartFormatting] - If true, the service converts dates, times, series of digits and numbers, phone numbers, currency values, and internet addresses into more readable, conventional representations (default=false)\n * @param {boolean} [options.speakerLabels] - If true, the response includes labels that identify which words were spoken by which participants in a multi-person exchange (default=false)\n * @param {string} [options.grammarName] - The name of a grammar that is to be used with the recognition request\n * @param {boolean} [options.redaction] - If true, the service redacts, or masks, numeric data from final transcripts (default=false)\n * @param {boolean} [options.processingMetrics] - If true, requests processing metrics about the service's transcription of the input audio (default=false)\n * @param {number} [options.processingMetricsInterval] - Specifies the interval in seconds at which the service is to return processing metrics\n * @param {boolean} [options.audioMetrics] - If true, requests detailed information about the signal characteristics of the input audio (detailed=false)\n *\n * @constructor\n */\nfunction RecognizeStream(options) {\n  // this stream only supports objectMode on the output side.\n  // It must receive binary data input.\n  if (options.objectMode) {\n    options.readableObjectMode = true;\n    delete options.objectMode;\n  }\n  Duplex.call(this, options);\n  this.options = options;\n  this.listening = false;\n  this.initialized = false;\n  this.finished = false;\n\n  this.on('newListener', function(event) {\n    if (!options.silent) {\n      if (event === 'results' || event === 'result' || event === 'speaker_labels') {\n        // eslint-disable-next-line no-console\n        console.log(\n          new Error(\n            'Watson Speech to Text RecognizeStream: the ' +\n              event +\n              ' event was deprecated. ' +\n              \"Please set {objectMode: true} and listen for the 'data' event instead. \" +\n              'Pass {silent: true} to disable this message.'\n          )\n        );\n      } else if (event === 'connection-close') {\n        // eslint-disable-next-line no-console\n        console.log(\n          new Error(\n            'Watson Speech to Text RecognizeStream: the ' +\n              event +\n              ' event was deprecated. ' +\n              \"Please listen for the 'close' event instead. \" +\n              'Pass {silent: true} to disable this message.'\n          )\n        );\n      } else if (event === 'connect') {\n        // eslint-disable-next-line no-console\n        console.log(\n          new Error(\n            'Watson Speech to Text RecognizeStream: the ' +\n              event +\n              ' event was deprecated. ' +\n              \"Please listen for the 'open' event instead. \" +\n              'Pass {silent: true} to disable this message.'\n          )\n        );\n      }\n    }\n  });\n}\nutil.inherits(RecognizeStream, Duplex);\n\nRecognizeStream.WEBSOCKET_CONNECTION_ERROR = 'WebSocket connection error';\n\nRecognizeStream.prototype.initialize = function() {\n  var options = this.options;\n\n  if (options.token && !options['watsonToken']) {\n    options['watsonToken'] = options.token;\n  }\n  if (options.content_type && !options['contentType']) {\n    options['contentType'] = options.content_type;\n  }\n  if (options['X-WDC-PL-OPT-OUT'] && !options['xWatsonLearningOptOut']) {\n    options['xWatsonLearningOptOut'] = options['X-WDC-PL-OPT-OUT'];\n  }\n\n  // compatibility code for the deprecated param, customization_id\n  if (options.customization_id && !options.languageCustomizationId) {\n    options.languageCustomizationId = options.customization_id;\n    delete options.customization_id;\n  }\n\n  // process query params\n  var queryParamsAllowed = [\n    'model',\n    'access_token',\n    'watson-token',\n    'language_customization_id',\n    'acoustic_customization_id',\n    'base_model_version',\n    'x-watson-learning-opt-out',\n    'x-watson-metadata'\n  ];\n  var queryParams = processUserParameters(options, queryParamsAllowed);\n  if (!queryParams.language_customization_id && !queryParams.model) {\n    queryParams.model = 'en-US_BroadbandModel';\n  }\n  var queryString = qs.stringify(queryParams);\n\n  var url = (options.url || 'wss://stream.watsonplatform.net/speech-to-text/api').replace(/^http/, 'ws') + '/v1/recognize?' + 'model=' + queryParams.model + '&access_token=' + queryParams.access_token;\n\n  // process opening payload params\n  var openingMessageParamsAllowed = [\n    'customization_weight',\n    'processing_metrics',\n    'processing_metrics_interval',\n    'audio_metrics',\n    'inactivity_timeout',\n    'timestamps',\n    'word_confidence',\n    'content-type',\n    'interim_results',\n    'keywords',\n    'keywords_threshold',\n    'max_alternatives',\n    'word_alternatives_threshold',\n    'profanity_filter',\n    'smart_formatting',\n    'speaker_labels',\n    'grammar_name',\n    'redaction'\n  ];\n  var openingMessage = processUserParameters(options, openingMessageParamsAllowed);\n  openingMessage.action = 'start';\n\n  var self = this;\n\n  // node params: requestUrl, protocols, origin, headers, extraRequestOptions\n  // browser params: requestUrl, protocols (all others ignored)\n  var socket = (this.socket = new W3CWebSocket(url, null, null, options.headers, null));\n\n  // when the input stops, let the service know that we're done\n  self.on('finish', self.finish.bind(self));\n\n  /**\n   * This can happen if the credentials are invalid - in that case, the response from DataPower doesn't include the\n   * necessary CORS headers, so JS can't even read it :(\n   *\n   * @param {Event} event - event object with essentially no useful information\n   */\n  socket.onerror = function(event) {\n    self.listening = false;\n    var err = new Error('WebSocket connection error');\n    err.name = RecognizeStream.WEBSOCKET_CONNECTION_ERROR;\n    err.event = event;\n    self.emit('error', err);\n    self.push(null);\n  };\n\n  this.socket.onopen = function() {\n    self.sendJSON(openingMessage);\n    /**\n     * emitted once the WebSocket connection has been established\n     * @event RecognizeStream#open\n     */\n    self.emit('open');\n  };\n\n  this.socket.onclose = function(e) {\n    // if (self.listening) {\n    self.listening = false;\n    self.push(null);\n    // }\n    /**\n     * @event RecognizeStream#close\n     * @param {Number} reasonCode\n     * @param {String} description\n     */\n    self.emit('close', e.code, e.reason);\n  };\n\n  /**\n   * @event RecognizeStream#error\n   * @param {String} msg custom error message\n   * @param {*} [frame] unprocessed frame (should have a .data property with either string or binary data)\n   * @param {Error} [err]\n   */\n  function emitError(msg, frame, err) {\n    if (err) {\n      err.message = msg + ' ' + err.message;\n    } else {\n      err = new Error(msg);\n    }\n    err.raw = frame;\n    self.emit('error', err);\n  }\n\n  socket.onmessage = function(frame) {\n    if (typeof frame.data !== 'string') {\n      return emitError('Unexpected binary data received from server', frame);\n    }\n\n    var data;\n    try {\n      data = JSON.parse(frame.data);\n    } catch (jsonEx) {\n      return emitError('Invalid JSON received from service:', frame, jsonEx);\n    }\n\n    /**\n     * Emit any messages received over the wire, mainly used for debugging.\n     *\n     * @event RecognizeStream#message\n     * @param {Object} message - frame object with a data attribute that's either a string or a Buffer/TypedArray\n     * @param {Object} [data] - parsed JSON object (if possible);\n     */\n    self.emit('message', frame, data);\n\n    if (data.error) {\n      emitError(data.error, frame);\n    } else if (data.state === 'listening') {\n      // this is emitted both when the server is ready for audio, and after we send the close message to indicate that it's done processing\n      if (self.listening) {\n        self.listening = false;\n        socket.close();\n      } else {\n        self.listening = true;\n        /**\n         * Emitted when the Watson Service indicates readiness to transcribe audio. Any audio sent before this point will be buffered until now.\n         * @event RecognizeStream#listening\n         */\n        self.emit('listening');\n      }\n    } else {\n      if (options.readableObjectMode) {\n        /**\n         * Object with interim or final results, possibly including confidence scores, alternatives, and word timing.\n         * @event RecognizeStream#data\n         * @param {Object} data\n         */\n        self.push(data);\n      } else if (Array.isArray(data.results)) {\n        data.results.forEach(function(result) {\n          if (result.final && result.alternatives) {\n            /**\n             * Finalized text\n             * @event RecognizeStream#data\n             * @param {String} transcript\n             */\n            self.push(result.alternatives[0].transcript, 'utf8');\n          }\n        });\n      }\n    }\n  };\n\n  this.initialized = true;\n};\n\nRecognizeStream.prototype.sendJSON = function sendJSON(msg) {\n  /**\n   * Emits any JSON object sent to the service from the client. Mainly used for debugging.\n   * @event RecognizeStream#send-json\n   * @param {Object} msg\n   */\n  this.emit('send-json', msg);\n  return this.socket.send(JSON.stringify(msg));\n};\n\nRecognizeStream.prototype.sendData = function sendData(data) {\n  /**\n   * Emits any Binary object sent to the service from the client. Mainly used for debugging.\n   * @event RecognizeStream#send-data\n   * @param {Object} msg\n   */\n  this.emit('send-data', data);\n  return this.socket.send(data);\n};\n\nRecognizeStream.prototype._read = function() /* size*/ {\n  // there's no easy way to control reads from the underlying library\n  // so, the best we can do here is a no-op\n};\n\nRecognizeStream.ERROR_UNRECOGNIZED_FORMAT = 'UNRECOGNIZED_FORMAT';\n\nRecognizeStream.prototype._write = function(chunk, encoding, callback) {\n  var self = this;\n  if (self.finished) {\n    // can't send any more data after the stop message (although this shouldn't happen normally...)\n    return;\n  }\n  if (!this.initialized) {\n    if (!this.options.contentType) {\n      var ct = RecognizeStream.getContentType(chunk);\n      if (ct) {\n        this.options.contentType = ct;\n      } else {\n        var err = new Error('Unable to determine content-type from file header, please specify manually.');\n        err.name = RecognizeStream.ERROR_UNRECOGNIZED_FORMAT;\n        this.emit('error', err);\n        this.push(null);\n        return;\n      }\n    }\n    this.initialize();\n\n    this.once('open', function() {\n      self.sendData(chunk);\n      self.afterSend(callback);\n    });\n  } else {\n    self.sendData(chunk);\n    this.afterSend(callback);\n  }\n};\n\n/**\n * Flow control - don't ask for more data until we've finished what we have\n *\n * Notes:\n *\n * This limits upload speed to 100 * options.highWaterMark / second.\n *\n * The default highWaterMark is 16kB, so the default max upload speed is ~1.6MB/s.\n *\n * Microphone input provides audio at a (downsampled) rate of:\n *   16000 samples/s * 16-bits * 1 channel = 32kB/s\n * (note the bits to Bytes conversion there)\n *\n * @private\n * @param {Function} next\n */\nRecognizeStream.prototype.afterSend = function afterSend(next) {\n  if (this.socket.bufferedAmount <= (this._writableState.highWaterMark || 0)) {\n    process.nextTick(next);\n  } else {\n    setTimeout(this.afterSend.bind(this, next), 10);\n  }\n};\n\n/**\n * Prevents any more audio from being sent over the WebSocket and gracefully closes the connection.\n * Additional data may still be emitted up until the `end` event is triggered.\n */\nRecognizeStream.prototype.stop = function() {\n  /**\n   * Event emitted when the stop method is called. Mainly for synchronising with file reading and playback.\n   * @event RecognizeStream#stop\n   */\n  this.emit('stop');\n  this.finish();\n};\n\nRecognizeStream.prototype.finish = function finish() {\n  // this is called both when the source stream finishes, and when .stop() is fired, but we only want to send the stop message once.\n  if (this.finished) {\n    return;\n  }\n  this.finished = true;\n  var self = this;\n  var closingMessage = { action: 'stop' };\n  if (self.socket && self.socket.readyState === self.socket.OPEN) {\n    self.sendJSON(closingMessage);\n  } else {\n    this.once('open', function() {\n      self.sendJSON(closingMessage);\n    });\n  }\n};\n\nRecognizeStream.prototype.promise = require('./to-promise');\n\nRecognizeStream.getContentType = function(buffer) {\n  // the substr really shouldn't be necessary, but there's a bug somewhere that can cause buffer.slice(0,4) to return\n  // the entire contents of the buffer, so it's a failsafe to catch that\n  return contentType.fromHeader(buffer);\n};\n\nmodule.exports = RecognizeStream;\n","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\result-stream.js",["96"],"/**\n * Copyright 2014 IBM Corp. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n'use strict';\n\nvar { Transform } = require('readable-stream');\nvar util = require('util');\nvar clone = require('clone');\n\n/**\n * Object-Mode stream that pulls result objects from the results array\n *\n * Also copies the top-level result_index to the individual results as .index\n *\n * @constructor\n * @param {Object} options\n */\nfunction ResultStream(options) {\n  options = options || {};\n  options.objectMode = true;\n  Transform.call(this, options);\n}\nutil.inherits(ResultStream, Transform);\n\nResultStream.prototype._transform = function(data, encoding, next) {\n  // when speaker_labels is enabled, some messages won't have a results array\n  if (Array.isArray(data.results)) {\n    // usually there is exactly 1 result, but there can be 0 in some circumstances, and potentially more in future iterations\n    data.results.forEach(function(result) {\n      var cloned = clone(result);\n      cloned.index = data.result_index;\n      this.push(cloned);\n    }, this);\n  } else {\n    this.push(data);\n  }\n  next();\n};\n\nResultStream.prototype.promise = require('./to-promise');\n\nmodule.exports = ResultStream;\n",["97","98"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\format-stream.js",["99"],"'use strict';\n\nvar { Transform } = require('readable-stream');\nvar util = require('util');\nvar clone = require('clone');\nvar defaults = require('defaults');\n\n/**\n * Applies some basic formatting to transcriptions:\n *  - Capitalize the first word of each sentence\n *  - Add a period to the end\n *  - Fix any \"cruft\" in the transcription\n *  - etc.\n *\n *  May be used as either a Stream, or a standalone helper.\n *\n * @param {Object} opts\n * @param {String} [opts.model] - some models / languages need special handling\n * @param {String} [opts.hesitation=''] - what to put down for a \"hesitation\" event, also consider \\u2026 (ellipsis: ...)\n * @param {Boolean} [options.objectMode=false] - emit `result` objects instead of string Buffers for the `data` events.\n * @constructor\n */\nfunction FormatStream(opts) {\n  this.options = defaults(opts, {\n    model: '', // some models should have all spaces removed\n    hesitation: '',\n    decodeStrings: false // false = don't convert strings to buffers before passing to _write\n  });\n  Transform.call(this, this.options);\n\n  this.isJaCn = this.options.model.substring(0, 5) === 'ja-JP' || this.options.model.substring(0, 5) === 'zh-CN';\n  this._transform = this.options.objectMode ? this.transformObject : this.transformString;\n}\nutil.inherits(FormatStream, Transform);\n\nvar reHesitation = /%HESITATION ?/g; // https://console.bluemix.net/docs/services/speech-to-text/output.html#output - D_ is handled below\nvar reRepeatedCharacter = /([a-z])\\1{2,}/gi; // detect the same character repeated three or more times and remove it\nvar reDUnderscoreWords = /D_[^\\s]+/g; // replace D_(anything)\n\n/**\n * Formats one or more words, removing special symbols, junk, and spacing for some languages\n * @param {String} text\n * @param {Boolean} isFinal\n * @return {String}\n */\nFormatStream.prototype.clean = function clean(text) {\n  // clean out \"junk\"\n  text = text\n    .replace(reHesitation, this.options.hesitation ? this.options.hesitation.trim() + ' ' : this.options.hesitation)\n    .replace(reRepeatedCharacter, '')\n    .replace(reDUnderscoreWords, '');\n\n  // remove spaces for Japanese and Chinese\n  if (this.isJaCn) {\n    text = text.replace(/ /g, '');\n  }\n\n  return text.trim() + ' '; // we want exactly 1 space at the end\n};\n\n/**\n * Capitalizes the first word of a sentence\n * @param {String} text\n * @return {string}\n */\nFormatStream.prototype.capitalize = function capitalize(text) {\n  // capitalize first word, returns '' in the case of an empty word\n  return text.charAt(0).toUpperCase() + text.substring(1);\n};\n\n/**\n * Puts a period on the end of a sentence\n * @param {String} text\n * @return {string}\n */\nFormatStream.prototype.period = function period(text) {\n  text = text.trim();\n  // don't put a period down if the clean stage remove all of the text\n  if (!text) {\n    return ' ';\n  }\n  // just add a space if the sentence ends in an ellipse\n  if (text.substr(-1) === '\\u2026') {\n    return text + ' ';\n  }\n  return text + (this.isJaCn ? 'ã€‚' : '. ');\n};\n\nFormatStream.prototype.transformString = function(chunk, encoding, next) {\n  this.push(this.formatString(chunk.toString()));\n  next();\n};\n\nFormatStream.prototype.transformObject = function formatResult(result, encoding, next) {\n  this.push(this.formatResult(result));\n  next();\n};\n\n/**\n * Formats a single string result.\n *\n * May be used outside of Node.js streams\n *\n * @param {String} str - text to format\n * @param {bool} [isInterim=false] - set to true to prevent adding a period to the end of the sentence\n * @return {String}\n */\nFormatStream.prototype.formatString = function(str, isInterim) {\n  str = this.capitalize(this.clean(str));\n  return isInterim ? str : this.period(str);\n};\n\n/**\n * Creates a new result with all transcriptions formatted\n *\n * May be used outside of Node.js streams\n *\n * @param {Object} data\n * @return {Object}\n */\nFormatStream.prototype.formatResult = function formatResult(data) {\n  data = clone(data);\n  if (Array.isArray(data.results)) {\n    data.results.forEach(function(result, i) {\n      // if there are multiple interim results (as produced by the speaker stream),\n      // treat the text as final in all but the last result\n      var textFinal = result.final || i !== data.results.length - 1;\n\n      result.alternatives = result.alternatives.map(function(alt) {\n        alt.transcript = this.formatString(alt.transcript, !textFinal);\n        if (alt.timestamps) {\n          alt.timestamps = alt.timestamps\n            .map(function(ts, j, arr) {\n              // timestamps is an array of arrays, each sub-array is in the form [\"word\", startTime, endTime]'\n              ts[0] = this.clean(ts[0]);\n              if (j === 0) {\n                ts[0] = this.capitalize(ts[0]);\n              }\n\n              if (j === arr.length - 1 && textFinal) {\n                ts[0] = this.period(ts[0]);\n              }\n              return ts;\n            }, this)\n            .filter(function(ts) {\n              return ts[0]; // remove any timestamps without a word (due to cleaning out junk words)\n            });\n        }\n        return alt;\n      }, this);\n    }, this);\n  }\n  return data;\n};\n\nFormatStream.prototype.promise = require('./to-promise');\n\nmodule.exports = FormatStream;\n",["100","101"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\index.js",[],["102","103"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\components\\ModelDropdown.js",[],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\renderer.js",["104","105"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\reportWebVitals.js",[],["106","107"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\util\\querystring.js",["108"],"'use strict';\n\n/**\n * Stringify query params, Watson-style\n *\n * Why? The server that processes auth tokens currently only accepts the *exact* string, even if it's invalid for a URL.\n * Properly url-encoding percent characters causes it to reject the token.\n * So, this is a custom qs.stringify function that properly encodes everything except watson-token, passing it along verbatim\n *\n * @param {Object} queryParams\n * @return {String}\n */\nexports.stringify = function stringify(queryParams) {\n  return Object.keys(queryParams)\n    .map(function(key) {\n      return key + '=' + (key === 'watson-token' ? queryParams[key] : encodeURIComponent(queryParams[key])); // the server chokes if the token is correctly url-encoded\n    })\n    .join('&');\n};\n",["109","110"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\content-type.js",["111"],"'use strict';\n\nvar extname = require('path').extname;\n\n// This module attempts to identify common content-types based on the filename or header.\n// It is not exhaustive, and for best results, you should always manually specify the content-type option.\n// See the complete list of supported content-types at\n// https://console.bluemix.net/docs/services/speech-to-text/input.html#formats\n\n// *some* file types can be identified by the first 3-4 bytes of the file\nvar headerContentTypes = {\n  fLaC: 'audio/flac',\n  RIFF: 'audio/wav',\n  OggS: 'audio/ogg',\n  ID3: 'audio/mp3',\n  '\\u001aEß£': 'audio/webm' // String for first four hex's of webm: [1A][45][DF][A3] (https://www.matroska.org/technical/specs/index.html#EBML)\n};\n\n/**\n * Takes the beginning of an audio file and returns the associated content-type / mime type\n *\n * @param {Buffer} buffer with at least the first 4 bytes of the file\n * @return {String|undefined} - the contentType of undefined\n */\nexports.fromHeader = function contentTypeFromHeader(buffer) {\n  var headerStr = buffer\n    .slice(0, 4)\n    .toString()\n    .substr(0, 4);\n  // mp3's are only consistent for the first 3 characters\n  return headerContentTypes[headerStr] || headerContentTypes[headerStr.substr(0, 3)];\n};\n\nvar filenameContentTypes = {\n  '.mp3': 'audio/mp3',\n  '.wav': 'audio/wav',\n  '.flac': 'audio/flac',\n  '.ogg': 'audio/ogg',\n  '.oga': 'audio/ogg',\n  '.opus': 'audio/ogg; codec=opus',\n  '.webm': 'audio/webm'\n};\n\n/**\n * Guess the content type from the filename\n *\n * Note: Blob and File objects include a .type property, but we're ignoring it because it's frequently either\n * incorrect (e.g. video/ogg instead of audio/ogg) or else a different format than what's expected (e.g. audio/x-wav)\n *\n * @param {String|Blob|File} file - string filename or url, or binary File/Blob object\n * @return {String|undefined}\n */\nexports.fromFilename = function contentTypeFromFilename(file) {\n  // todo: consider retrying with querystring & hash stripped from URLs\n  var ext = extname((typeof file === 'string' && file) || file.name || '');\n  return filenameContentTypes[ext];\n};\n",["112","113"],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\no-timestamps.js",["114"],"'use strict';\n\n/**\n * Returns true if the result is missing it's timestamps\n * @param {Object} data\n * @return {Boolean}\n */\nmodule.exports = function noTimestamps(data) {\n  return data.results.some(function(result) {\n    var alt = result.alternatives && result.alternatives[0];\n    return !!(alt && ((alt.transcript.trim() && !alt.timestamps) || !alt.timestamps.length));\n  });\n};\n\nmodule.exports.ERROR_NO_TIMESTAMPS = 'NO_TIMESTAMPS';\n","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\writable-element-stream.js",["115"],"'use strict';\n\nvar { Writable } = require('readable-stream');\nvar util = require('util');\nvar defaults = require('defaults');\n\n/**\n * Writable stream that accepts results in either object or string mode and outputs the text to a supplied html element\n *\n * Can show interim results when in objectMode\n *\n * @param {Object} options\n * @param {String|DOMElement} options.outputElement\n * @param {String} [options.property] what property of the element should the text be set to. Defaults to `value` for `<input>`s and `<textarea>`s, `textContent` for everything else\n * @param {Boolean} [options.clear=true] delete any previous text\n * @constructor\n */\nfunction WritableElementStream(options) {\n  this.options = options = defaults(options, {\n    decodeStrings: false, // false = don't convert strings to buffers before passing to _write (only applies in string mode)\n    property: null,\n    clear: true\n  });\n\n  this.el = typeof options.outputElement === 'string' ? document.querySelector(options.outputElement) : options.outputElement;\n\n  if (!this.el) {\n    throw new Error('Watson Speech to Text WritableElementStream: missing outputElement');\n  }\n\n  Writable.call(this, options);\n\n  // for most elements we set the textContent, but for form elements, the value property is probably the expected target\n  var propMap = {\n    INPUT: 'value',\n    TEXTAREA: 'value'\n  };\n  this.prop = options.property || propMap[this.el.nodeName] || 'textContent';\n\n  if (options.clear) {\n    this.el[this.prop] = '';\n  }\n\n  if (options.objectMode) {\n    this.finalizedText = this.el[this.prop];\n    this._write = this.writeObject;\n  } else {\n    this._write = this.writeString;\n  }\n}\nutil.inherits(WritableElementStream, Writable);\n\nWritableElementStream.prototype.writeString = function writeString(text, encoding, next) {\n  this.el[this.prop] += text;\n  next();\n};\n\nWritableElementStream.prototype.writeObject = function writeObject(data, encoding, next) {\n  if (Array.isArray(data.results)) {\n    data.results.forEach(function(result) {\n      if (result.final) {\n        this.finalizedText += result.alternatives[0].transcript;\n        this.el[this.prop] = this.finalizedText;\n      } else {\n        this.el[this.prop] = this.finalizedText + result.alternatives[0].transcript;\n      }\n    }, this);\n  }\n  next();\n};\n\nmodule.exports = WritableElementStream;\n","D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\util\\process-user-parameters.js",[],"D:\\Lab\\speech-to-text-electron\\electron-react-app\\src\\lib\\speech-to-text\\to-promise.js",["116"],{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"121"},{"ruleId":"117","severity":1,"message":"118","line":17,"column":1,"nodeType":"119","messageId":"120","endLine":17,"endColumn":14,"fix":"122"},{"ruleId":"123","replacedBy":"124"},{"ruleId":"125","replacedBy":"126"},{"ruleId":"117","severity":1,"message":"118","line":17,"column":1,"nodeType":"119","messageId":"120","endLine":17,"endColumn":14,"fix":"127"},{"ruleId":"128","severity":1,"message":"129","line":44,"column":5,"nodeType":"130","messageId":"131","endLine":44,"endColumn":12},{"ruleId":"128","severity":1,"message":"132","line":45,"column":5,"nodeType":"130","messageId":"131","endLine":45,"endColumn":17},{"ruleId":"117","severity":1,"message":"118","line":17,"column":1,"nodeType":"119","messageId":"120","endLine":17,"endColumn":14,"fix":"133"},{"ruleId":"128","severity":1,"message":"134","line":159,"column":7,"nodeType":"130","messageId":"131","endLine":159,"endColumn":18},{"ruleId":"135","severity":1,"message":"136","line":161,"column":125,"nodeType":"137","messageId":"138","endLine":161,"endColumn":126},{"ruleId":"117","severity":1,"message":"118","line":17,"column":1,"nodeType":"119","messageId":"120","endLine":17,"endColumn":14,"fix":"139"},{"ruleId":"123","replacedBy":"140"},{"ruleId":"125","replacedBy":"141"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"142"},{"ruleId":"123","replacedBy":"143"},{"ruleId":"125","replacedBy":"144"},{"ruleId":"123","replacedBy":"145"},{"ruleId":"125","replacedBy":"146"},{"ruleId":"128","severity":1,"message":"147","line":4,"column":7,"nodeType":"130","messageId":"131","endLine":4,"endColumn":19},{"ruleId":"128","severity":1,"message":"148","line":11,"column":10,"nodeType":"130","messageId":"131","endLine":11,"endColumn":25},{"ruleId":"123","replacedBy":"149"},{"ruleId":"125","replacedBy":"150"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"151"},{"ruleId":"123","replacedBy":"152"},{"ruleId":"125","replacedBy":"153"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"154"},{"ruleId":"123","replacedBy":"155"},{"ruleId":"125","replacedBy":"156"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"157"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"158"},{"ruleId":"117","severity":1,"message":"118","line":1,"column":1,"nodeType":"119","messageId":"120","endLine":1,"endColumn":14,"fix":"159"},"strict","'use strict' is unnecessary inside of modules.","ExpressionStatement","module",{"range":"160","text":"161"},{"range":"162","text":"161"},"no-native-reassign",["163"],"no-negated-in-lhs",["164"],{"range":"165","text":"161"},"no-unused-vars","'seq_num' is assigned a value but never used.","Identifier","unusedVar","'languageCode' is assigned a value but never used.",{"range":"166","text":"161"},"'queryString' is assigned a value but never used.","no-useless-concat","Unexpected string concatenation of literals.","BinaryExpression","unexpectedConcat",{"range":"167","text":"161"},["163"],["164"],{"range":"168","text":"161"},["163"],["164"],["163"],["164"],"'trackElement' is assigned a value but never used.","'initialize_test' is defined but never used.",["163"],["164"],{"range":"169","text":"161"},["163"],["164"],{"range":"170","text":"161"},["163"],["164"],{"range":"171","text":"161"},{"range":"172","text":"161"},{"range":"173","text":"161"},[0,13],"",[617,630],"no-global-assign","no-unsafe-negation",[617,630],[617,630],[617,630],[0,13],[0,13],[0,13],[0,13],[0,13],[0,13]]